===== Scratch Disks with ZFS Stripe =====

One common use case for large network storage spaces are scratch disks for scientific and engineering calculations, for video editing, or other applications where large temporary storage spaces are needed. In these cases, redundancy is not important as the data has no long-term purpose; for these applications we are interested in the excellent read/write performance of ZFS striped arrays. With fast storage controllers and network interfaces, proper configuration can saturate their capabilities and deliver impressive I/O performance. This application's analogous technology is the RAID 0, a simple striped storage pool with no parity.

A simple ZFS striped pool is the default pool created when a redundancy level is not specified with //zpool create//, the command for creating storage pools from specified devices. We'll configure our first storage server with several of its disks as a ZFS striped pool. Be sure to take a VM snapshot so we can roll this back and try another configuration later.

The //zpool create// command will be used here specifying the pool name (scratchy), the //-o// option which lets us specify a zpool property ([[principlesofzfs#additional_considerations|ashift=12]]), and the devices we'd like to include. You can identify these drives by anything they go by in // /dev/ // as well as in // /dev/disk/ //, and how you specify them is how //zpool// commands will report them. We'd like to know our disks by partition label, so we will specify the symlinks in // /dev/disk/by-partlabel/ //.

<code># zpool create scratchy -o ashift=12 /dev/disk/by-partlabel/zfs{0..5}</code>
Here we've used a feature of our shell to run the command adding the first six devices because we know exactly how we are laid out. It's also important to know that ashift can only be set when creating the storage pool.

Check out the results of this command with //zpool status//.
<code># zpool status
  pool: scratchy
 state: ONLINE
  scan: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	scratchy    ONLINE       0     0     0
	  zfs0      ONLINE       0     0     0
	  zfs1      ONLINE       0     0     0
	  zfs2      ONLINE       0     0     0
	  zfs3      ONLINE       0     0     0
	  zfs4      ONLINE       0     0     0
	  zfs5      ONLINE       0     0     0

errors: No known data errors</code>

ZFS makes this output very understandable. Of note in this example is the name column, which will be arranged as a tree of ZFS' fundamental logical storage unit, the virtual device (VDEV). VDEVs are a unit of one or more storage devices, which are also VDEVs themselves. There are a few different types of VDEVs in the ZFS storage model, and we will describe these as we encounter them in examples.

This very simple storage pool is composed of twelve VDEVs. Each of these VDEVs are the zfs0 through 11 partitions across each of our disks. The storage pool is the top layer of our array, which represents the storage space distributed across our VDEVs. The default stripe demonstrated here has no redundancy, so if a single disk in this pool fails, the entire pool is lost. Since this is scratch space for calculations, we won't miss the data in the event that we need to replace the failed disk and rebuild a new pool. ZFS even has the ability to keep hot spares installed as failovers, but they would not save this array if a disk were to fault.

Let's examine some storage arrays which use the redundancy features of ZFS. We can destroy this pool with the command <code># zpool destroy scratchy</code> or roll back a previous VM snapshot.

==== Hot Spares ====

ZFS has the ability to keep spare devices on standby to swap in when one in an active zpool fails. The spare can be swapped out manually, but automatically failing to a hot spare is a documented feature of ZFS as well. ZoL documentation that confirms automatic hot spare swapping is not available, so we should test if it works. Take a VM snapshot and we'll give this a try.

Let's add the last disk attached to our VM. Our example VM's last device, /dev/sdn, wasn't partitioned earlier. Do so with:
<code>
# parted --script --align optimal /dev/sdn mklabel gpt mkpart zfs13 0% 19000MB mkpart linux-swap 19000MB 100% print
Model: VBOX HARDDISK (scsi)
Disk /dev/sdn: 21.5GB
Sector size (logical/physical): 512B/512B
Partition Table: gpt
Disk Flags:

Number  Start   End     Size    File system  Name        Flags
 1      1049kB  19.0GB  19.0GB               zfs13
 2      19.0GB  21.5GB  2474MB               linux-swap
</code>

Let's add this device as a spare to scratchy. The zpool command syntax for adding devices is:
<code>
zpool add [-fn] [-o property=value] pool vdev ...
</code>
...and the command for our case is shown below with a //zpool status// output that shows our spare.

<code>
# zpool add scratchy spare /dev/disk/by-partlabel/zfs12
# zpool status
  pool: scratchy
 state: ONLINE
  scan: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	scratchy    ONLINE       0     0     0
	  zfs1      ONLINE       0     0     0
	  zfs2      ONLINE       0     0     0
	  zfs3      ONLINE       0     0     0
	  zfs4      ONLINE       0     0     0
	  zfs5      ONLINE       0     0     0
	  zfs6      ONLINE       0     0     0
	spares
	  zf12      AVAIL

</code>

We have omitted setting the ashift as our setting applied to the entire pool when it was created. Remember that ashift can't be adjusted after creating the pool.

==== RAID stripes Vs. ZFS Stripes ====

Traditional RAID stripes spreads pieces of data across the numerous devices in the RAID 0, generally in uniform 128KB chunks parallel across all devices. ZFS, on the other hand, provides dynamic stripe sizing that adapts to the data, hardware, and workload. ZFS stripe sizes can vary non-uniformly up to 1MB on some systems, depending on the data's projected read and write performance.

ZFS stripes are also flexible in their device composition. Traditional RAID stripes are fixed with device sizes and count. A RAID 0 with four 3TB disks will always have four disks with 12GB of storage, and while you can replace the 3TB disks with 4TB disks, the available storage will remain at 12GB unless the array is rebuilt. ZFS dynamically expands with number of disks and sizes of those disks so that everything is utilized. This eliminates the great amount of labor in expanding traditional storage arrays: backing up the existing data, destroying the array, rebuilding it with the new size, then restoring the data. This principle is applied to all redundancy levels of ZFS simply by adding VDEVs to the pool.
